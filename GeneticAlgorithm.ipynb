{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RqlnUGwHT_Gu"
   },
   "source": [
    "Libreries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "gIF9UubTT5sy"
   },
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import csv\n",
    "\n",
    "import matplotlib.pyplot as plt # plotting library\n",
    "# %matplotlib inline\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense , Activation, Dropout,BatchNormalization,Input\n",
    "from tensorflow.keras.optimizers import Adam ,RMSprop\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import  backend as K\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint,EarlyStopping\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV,ParameterGrid, ParameterSampler,train_test_split\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.metrics import mean_absolute_error,accuracy_score, balanced_accuracy_score\n",
    "\n",
    "import random\n",
    "from random import random,randrange\n",
    "from operator import itemgetter\n",
    "import timeit\n",
    "import random\n",
    "import os\n",
    "import pickle\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "initializer = tf.keras.initializers.GlorotUniform(seed=2)\n",
    "random.seed(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/gpu/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:30: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  method='lar', copy_X=True, eps=np.finfo(np.float).eps,\n",
      "/root/anaconda3/envs/gpu/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:167: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  method='lar', copy_X=True, eps=np.finfo(np.float).eps,\n",
      "/root/anaconda3/envs/gpu/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:284: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, copy_Gram=True, verbose=0,\n",
      "/root/anaconda3/envs/gpu/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:862: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, copy_X=True, fit_path=True,\n",
      "/root/anaconda3/envs/gpu/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:1101: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, copy_X=True, fit_path=True,\n",
      "/root/anaconda3/envs/gpu/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:1127: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, positive=False):\n",
      "/root/anaconda3/envs/gpu/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:1362: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  max_n_alphas=1000, n_jobs=None, eps=np.finfo(np.float).eps,\n",
      "/root/anaconda3/envs/gpu/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:1602: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  max_n_alphas=1000, n_jobs=None, eps=np.finfo(np.float).eps,\n",
      "/root/anaconda3/envs/gpu/lib/python3.7/site-packages/sklearn/linear_model/least_angle.py:1738: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  eps=np.finfo(np.float).eps, copy_X=True, positive=False):\n",
      "/root/anaconda3/envs/gpu/lib/python3.7/site-packages/sklearn/decomposition/online_lda.py:29: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  EPS = np.finfo(np.float).eps\n",
      "/root/anaconda3/envs/gpu/lib/python3.7/site-packages/sklearn/ensemble/gradient_boosting.py:32: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  from ._gradient_boosting import predict_stages\n",
      "/root/anaconda3/envs/gpu/lib/python3.7/site-packages/sklearn/ensemble/gradient_boosting.py:32: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  from ._gradient_boosting import predict_stages\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('./models/')\n",
    "from fcunet import fcunet_model\n",
    "from irnet import irnet_model\n",
    "from fcmnr import fcmnr_model\n",
    "\n",
    "from Metalearner import meta_learner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MQSFAbAdUHj2"
   },
   "source": [
    "Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/gpu/lib/python3.7/site-packages/sklearn/utils/__init__.py:806: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  return floored.astype(np.int)\n",
      "/root/anaconda3/envs/gpu/lib/python3.7/site-packages/sklearn/utils/__init__.py:806: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  return floored.astype(np.int)\n"
     ]
    }
   ],
   "source": [
    "bankruptcy_df = pd.read_csv(\"./data/bankruptcy_dataset.csv\")\n",
    "X = bankruptcy_df.iloc[:,1:]\n",
    "Y = bankruptcy_df.iloc[:,[0]]\n",
    "Y=to_categorical(Y)\n",
    "scaler = RobustScaler()\n",
    "X.iloc[:,:] = scaler.fit_transform(X)\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y , test_size=0.3, random_state=42, stratify=Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lyk2FMPXVEeZ"
   },
   "source": [
    "### Genetic Algorithm Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "fmdC3fNTz0nY"
   },
   "outputs": [],
   "source": [
    "#INITIALIZE POPULATION FOR 4 HPs\n",
    "def initialize_population(population_size,n_layers,learning_rate,batch_size,activation_function):\n",
    "\n",
    "  param_grid = dict(n_layers=n_layers,learning_rate=learning_rate,\n",
    "                    batch_size=batch_size,activation_function=activation_function)\n",
    "  grid_search_population=list(ParameterSampler(param_grid,population_size))\n",
    "\n",
    "  potential_n_layers=[]\n",
    "  potential_learning_rate=[]\n",
    "  potential_batch_size=[]\n",
    "  potential_activation_function=[]\n",
    "\n",
    "  for i in range(0,population_size):\n",
    "    potential_n_layers.insert(0,grid_search_population[i]['n_layers'])\n",
    "    potential_learning_rate.insert(0,grid_search_population[i]['learning_rate'])\n",
    "    potential_batch_size.insert(0,grid_search_population[i]['batch_size'])\n",
    "    potential_activation_function.insert(0,grid_search_population[i]['activation_function'])\n",
    "\n",
    "  return potential_n_layers,potential_learning_rate,potential_batch_size,potential_activation_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#EVALUATE FITNESS\n",
    "def evaluate_fitness(input_shape,n_layers,activation_function,learning_rate,batch_size,hp_dataset_name,max_epochs,patience_epochs,metric_to_evaluate):\n",
    "  #CREATE MODEL\n",
    "  \n",
    "  model=irnet_model(input_shape,n_layers,activation_function,learning_rate) \n",
    "\n",
    "  start_time = timeit.default_timer()\n",
    "  history = model.fit(x_train,y_train,\n",
    "                      batch_size=batch_size,\n",
    "                      epochs=max_epochs,\n",
    "                      validation_split=0.2,\n",
    "                      callbacks=[EarlyStopping(patience=patience_epochs)])\n",
    "  end_time = timeit.default_timer()\n",
    "\n",
    "  #EVALUATE MODEL\n",
    "  prediction=model.predict(x_test)\n",
    "  if(len(prediction.transpose())!=len(prediction)): # IF RESULT IS ONE-HOT ENCODED, CHANGE IT.\n",
    "    prediction=np.argmax(prediction,axis=1)\n",
    "\n",
    "  \n",
    "  if(metric_to_evaluate=='mae'): metric_test=mean_absolute_error(np.argmax(y_test,axis=1),prediction)\n",
    "  if(metric_to_evaluate=='accuracy'): metric_test=accuracy_score(np.argmax(y_test,axis=1),prediction)\n",
    "  if(metric_to_evaluate=='balanced_accuracy'): metric_test=balanced_accuracy_score(np.argmax(y_test,axis=1),prediction)\n",
    "  \n",
    "  #SAVE THE WEIGHTS\n",
    "  weights_name=\"{}-{}-{}-{}\".format(n_layers,input_shape,activation_function,learning_rate)\n",
    "  model.save(weights_folder+weights_name+\".h5\")\n",
    "\n",
    "  #SAVE THE HYPERPARAMS AND THE METRIC\n",
    "  with open(hp_dataset_name, mode='a+') as hp_dataset:\n",
    "      hp_dataset_writer=csv.writer(hp_dataset,delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "      hp_dataset_writer.writerow([architecture_name,\n",
    "                              problem_type,\n",
    "                              num_features,\n",
    "                              training_and_validation_samples,\n",
    "                              n_layers,\n",
    "                              input_shape,\n",
    "                              activation_function,\n",
    "                              learning_rate,\n",
    "                              batch_size,\n",
    "                              str(len(history.history['loss'])),\n",
    "                              end_time-start_time,\n",
    "                              metric_test\n",
    "                              ])\n",
    "  return metric_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def selection(evaluated_hparams,sel_prt,rand_prt,population, metric_to_evaluate,sort_order_desc):\n",
    "    \n",
    "    sorted_evaluated_params=sorted(evaluated_hparams,key=itemgetter('metric'),reverse=True)\n",
    "    if(sel_prt+rand_prt>=len(population[0])):\n",
    "      print(\"WARNING: Selections are bigger thant current population\")\n",
    "      print(\"WARNING: Random selection may not be taken\")\n",
    "\n",
    "    top_selection=[]\n",
    "    for i in range(sel_prt):\n",
    "      top_selection.insert(len(top_selection),sorted_evaluated_params[i]['hparam'])\n",
    "\n",
    "    rand_selection=[]\n",
    "    i=0\n",
    "    while(i < rand_prt):\n",
    "      if(len(rand_selection)+len(top_selection)>=len(population[0])):\n",
    "        break\n",
    "\n",
    "      rand_hparam=randrange(len(population[0]))\n",
    "      print(\"Generated random {}.\".format(rand_hparam))\n",
    "      if(rand_hparam in top_selection or rand_hparam in rand_selection):\n",
    "        continue\n",
    "\n",
    "      rand_selection.insert(0,rand_hparam)\n",
    "      i=i+1\n",
    "    return top_selection,rand_selection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CROSS-OVER OPERATION\n",
    "\n",
    "def crossover(p1,p2,population):\n",
    "    child_potential_n_layers=[]\n",
    "    child_potential_learning_rate=[]\n",
    "    child_potential_batch_size=[]\n",
    "    child_potential_activation_function=[]\n",
    "\n",
    "    #child1\n",
    "    child_potential_n_layers.insert(0,population[0][p1])\n",
    "    child_potential_learning_rate.insert(0,population[1][p2])\n",
    "    child_potential_batch_size.insert(0,population[2][p1])\n",
    "    child_potential_activation_function.insert(0,population[3][p2])\n",
    "\n",
    "    #child2\n",
    "    child_potential_n_layers.insert(0,population[0][p2])\n",
    "    child_potential_learning_rate.insert(0,population[1][p1])\n",
    "    child_potential_batch_size.insert(0,population[2][p2])\n",
    "    child_potential_activation_function.insert(0,population[3][p1])\n",
    "    \n",
    "    #child3\n",
    "    child_potential_n_layers.insert(0,population[0][p1])\n",
    "    child_potential_learning_rate.insert(0,population[1][p1])\n",
    "    child_potential_batch_size.insert(0,population[2][p2])\n",
    "    child_potential_activation_function.insert(0,population[3][p2])\n",
    "    \n",
    "    #child4\n",
    "    child_potential_n_layers.insert(0,population[0][p2])\n",
    "    child_potential_learning_rate.insert(0,population[1][p2])\n",
    "    child_potential_batch_size.insert(0,population[2][p1])\n",
    "    child_potential_activation_function.insert(0,population[3][p1])\n",
    "    \n",
    "    \n",
    "    child_hparams=[child_potential_n_layers,child_potential_learning_rate,child_potential_batch_size,child_potential_activation_function]\n",
    "    return child_hparams\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# MUTATION\n",
    "def mutation(population,selected):\n",
    "    selected_hyperparam=randrange(len(all_hyperparams))\n",
    "    selected_value=randrange(len(all_hyperparams[selected_hyperparam]))\n",
    "    population[selected_hyperparam][selected]=all_hyperparams[selected_hyperparam][selected_value]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAIN\n",
    "def genetic_algorithm_main(population_size,input_shape,hp_dataset_name,max_epochs,patience_epochs,metric_to_evaluate,sort_order_desc):\n",
    "    potential_n_layers,potential_learning_rate,potential_batch_size,potential_activation_function=initialize_population(population_size=population_size,\n",
    "                                                                                                n_layers=n_layers,\n",
    "                                                                                                learning_rate=learning_rate,\n",
    "                                                                                                batch_size=batch_size,\n",
    "                                                                                                activation_function=activation_function)\n",
    "    population=[potential_n_layers,potential_learning_rate,potential_batch_size,potential_activation_function]\n",
    "    print(\"Initial population\",population)\n",
    "    final_hyperparam=[]\n",
    "    # evaluate hyperparams\n",
    "    for generation in range(generations):\n",
    "        evaluated_hparams=[]\n",
    "        for i in range(population_size):\n",
    "            #input_shape,n_layers,activation_function,learning_rate,batch_size,hp_dataset_name,max_epochs,patience_epochs,metric_to_evaluate\n",
    "            metric=evaluate_fitness(input_shape=input_shape,\n",
    "                                    n_layers=population[0][i],\n",
    "                                    learning_rate=population[1][i],\n",
    "                                    batch_size=population[2][i],\n",
    "                                    activation_function=population[3][i],\n",
    "                                    hp_dataset_name=hp_dataset_name,\n",
    "                                    max_epochs=max_epochs,\n",
    "                                    patience_epochs=patience_epochs,\n",
    "                                    metric_to_evaluate=metric_to_evaluate)\n",
    "            evaluated_hparams.insert(0,{\"hparam\":i,\"metric\":metric})\n",
    "\n",
    "        #SELECTION\n",
    "        top_selection,rand_selection=selection(evaluated_hparams,sel_prt,rand_prt,population,metric_to_evaluate,sort_order_desc)\n",
    "\n",
    "\n",
    "        # CROSS-OVER\n",
    "        p1,p2=random.sample(range(0,len(top_selection)+len(rand_selection)),2)\n",
    "        child_hyperparams= (p1,p2,population)\n",
    "\n",
    "        #CREATE NEW POPULATION\n",
    "        #insert top selections\n",
    "        new_population=[[population[0][i] for i in top_selection],\n",
    "                          [population[1][i] for i in top_selection],\n",
    "                          [population[2][i] for i in top_selection],\n",
    "                          [population[3][i] for i in top_selection]]\n",
    "        #insert random selection and childs\n",
    "        new_population[0]=[*new_population[0],\n",
    "                                    *[population[0][i] for i in rand_selection],\n",
    "                                   *child_hyperparams[0]]\n",
    "        new_population[1]=[*new_population[1],\n",
    "                                    *[population[1][i] for i in rand_selection],\n",
    "                                   *child_hyperparams[1]]\n",
    "        new_population[2]=[*new_population[2],\n",
    "                                    *[population[2][i] for i in rand_selection],\n",
    "                                   *child_hyperparams[2]]\n",
    "        new_population[3]=[*new_population[3],\n",
    "                                    *[population[3][i] for i in rand_selection],\n",
    "                                   *child_hyperparams[3]]    \n",
    "\n",
    "        # MUTATION\n",
    "        selected_to_mutate=randrange(len(top_selection)+len(rand_selection)+len(child_hyperparams[0]))\n",
    "        mutation(new_population,selected_to_mutate)\n",
    "\n",
    "        if (generation+1)==generations:\n",
    "            for  hyperparam in  population:\n",
    "                final_hyperparam.insert(len(population),hyperparam[top_selection[0]])\n",
    "\n",
    "        population=new_population\n",
    "        population_size=len(population[0])\n",
    "\n",
    "    return evaluated_hparams,sorted(evaluated_hparams,key=itemgetter('metric'),reverse=sort_order_desc)[0]['metric'],final_hyperparam\n",
    "    \n",
    "                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/tesis/metalearner-hptunning/Metalearner.py:57: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  dataset_task_dim[\"metric\"] = pd.to_numeric(dataset_task_dim[\"metric\"])\n",
      "/root/anaconda3/envs/gpu/lib/python3.7/site-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "/root/anaconda3/envs/gpu/lib/python3.7/site-packages/sklearn/ensemble/base.py:158: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  dtype=np.int)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata loaded.\n",
      "Meta model created.\n",
      "HP space created.\n",
      "HP space predicted\n",
      "Top combination created.\n",
      "irnet [0.01  0.001]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'asd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-908f129d41bd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     32\u001b[0m                 to_categorical_column_names,data_file_name,num_features,training_samples,n_layers,learning_rate,batch_size,activation_function)\n\u001b[1;32m     33\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mselected_arch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtop_lr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m \u001b[0masd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'asd' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "#FILES NAME\n",
    "hp_dataset_name=\"test_hp_dataset.csv\"\n",
    "weights_folder=\"data/weights/\"\n",
    "#HYPERPARAMETERS TO EVALUATE\n",
    "num_features=x_train.shape[1]\n",
    "training_samples=x_train.shape[0]\n",
    "training_and_validation_samples=len(x_train)\n",
    "n_layers=[1,2,3]\n",
    "learning_rate=[0.01,0.001,0.0001,0.00001]\n",
    "batch_size=[16,32,64,128]\n",
    "activation_function=['relu','elu','tanh','sigmoid']\n",
    "\n",
    "\n",
    "# METALEARNER\n",
    "to_categorical_column_names=[\"activation_function\"] \n",
    "n_top_hp_to_select=2\n",
    "dataset_column_names=[\"architecture\",\"error_metric\",\"task\",\"num_features\",\n",
    "                    \"training_samples\",\"n_layers\",\"activation_function\",\n",
    "                    \"learning_rate\",\"batch_size\",\"metric\",\"dimension\",\"dataset\",\"y\"]\n",
    "\n",
    "x_column_names=[\"num_features\",\"training_samples\",\n",
    "                    \"n_layers\",\"activation_function\",\n",
    "                    \"learning_rate\", \"batch_size\"]\n",
    "\n",
    "to_categorical_column_names=[\"activation_function\"]\n",
    "data_file_name=\"data/metadataset.csv\"\n",
    "dnn_architecture=\"all\"\n",
    "dnn_task=\"prediction\"\n",
    "dnn_dim=1\n",
    "\n",
    "top_lr,top_bz,top_layers,top_af,finish_order,selected_arch=meta_learner(dnn_architecture,dnn_task,dnn_dim,n_top_hp_to_select,dataset_column_names,x_column_names,\n",
    "                to_categorical_column_names,data_file_name,num_features,training_samples,n_layers,learning_rate,batch_size,activation_function)\n",
    "print(selected_arch,top_lr)\n",
    "asd\n",
    "\n",
    "input_shape=x_train.shape[1]\n",
    "max_epochs=2\n",
    "patience_epochs=2\n",
    "metric_to_evaluate=\"balanced_accuracy\"\n",
    "sort_order_desc=True\n",
    "\n",
    "\n",
    "\n",
    "#GA configuration\n",
    "all_hyperparams=[n_layers,learning_rate,batch_size,activation_function]\n",
    "population_size=6\n",
    "sel_prt=2\n",
    "rand_prt=2\n",
    "generations=2\n",
    "all_ga,top_ga, hparams_ga=genetic_algorithm_main(population_size,\n",
    "                                                input_shape,\n",
    "                                                hp_dataset_name,\n",
    "                                                max_epochs,\n",
    "                                                patience_epochs,\n",
    "                                                metric_to_evaluate,\n",
    "                                                sort_order_desc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "NN MNIST  - GA.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.7.4 ('gpu': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "5e66b90ac85f38f39d9ade17082e1282e7794075483f5b8f4b36ae4e18b98d63"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
