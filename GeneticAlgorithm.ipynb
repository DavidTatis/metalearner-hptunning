{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RqlnUGwHT_Gu"
   },
   "source": [
    "Libreries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "gIF9UubTT5sy"
   },
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import csv\n",
    "\n",
    "import matplotlib.pyplot as plt # plotting library\n",
    "# %matplotlib inline\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense , Activation, Dropout,BatchNormalization,Input\n",
    "from tensorflow.keras.optimizers import Adam ,RMSprop\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import  backend as K\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint,EarlyStopping\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV,ParameterGrid, ParameterSampler,train_test_split\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.metrics import mean_absolute_error,accuracy_score, balanced_accuracy_score\n",
    "\n",
    "import random\n",
    "from random import random,randrange\n",
    "from operator import itemgetter\n",
    "import timeit\n",
    "import random\n",
    "import os\n",
    "import pickle\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "initializer = tf.keras.initializers.GlorotUniform(seed=2)\n",
    "random.seed(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('./models/')\n",
    "from fcunet import fcunet_model\n",
    "from irnet import irnet_model\n",
    "from fcmnr import fcmnr_model\n",
    "\n",
    "from Metalearner import meta_learner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MQSFAbAdUHj2"
   },
   "source": [
    "Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "bankruptcy_df = pd.read_csv(\"./data/bankruptcy_dataset.csv\")\n",
    "X = bankruptcy_df.iloc[:,1:]\n",
    "Y = bankruptcy_df.iloc[:,[0]]\n",
    "Y=to_categorical(Y)\n",
    "scaler = RobustScaler()\n",
    "X.iloc[:,:] = scaler.fit_transform(X)\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y , test_size=0.3, random_state=42, stratify=Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lyk2FMPXVEeZ"
   },
   "source": [
    "### Genetic Algorithm Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "fmdC3fNTz0nY"
   },
   "outputs": [],
   "source": [
    "#INITIALIZE POPULATION FOR 4 HPs\n",
    "def initialize_population(population_size,n_layers,learning_rate,batch_size,activation_function):\n",
    "\n",
    "  param_grid = dict(n_layers=n_layers,learning_rate=learning_rate,\n",
    "                    batch_size=batch_size,activation_function=activation_function)\n",
    "  grid_search_population=list(ParameterSampler(param_grid,population_size))\n",
    "\n",
    "  potential_n_layers=[]\n",
    "  potential_learning_rate=[]\n",
    "  potential_batch_size=[]\n",
    "  potential_activation_function=[]\n",
    "\n",
    "  for i in range(0,population_size):\n",
    "    potential_n_layers.insert(0,grid_search_population[i]['n_layers'])\n",
    "    potential_learning_rate.insert(0,grid_search_population[i]['learning_rate'])\n",
    "    potential_batch_size.insert(0,grid_search_population[i]['batch_size'])\n",
    "    potential_activation_function.insert(0,grid_search_population[i]['activation_function'])\n",
    "\n",
    "  return potential_n_layers,potential_learning_rate,potential_batch_size,potential_activation_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#EVALUATE FITNESS\n",
    "def evaluate_fitness(input_shape,n_layers,activation_function,learning_rate,batch_size,hp_dataset_name,max_epochs,patience_epochs,metric_to_evaluate):\n",
    "  #CREATE MODEL\n",
    "  \n",
    "  model=irnet_model(input_shape,n_layers,activation_function,learning_rate) \n",
    "\n",
    "  start_time = timeit.default_timer()\n",
    "  history = model.fit(x_train,y_train,\n",
    "                      batch_size=batch_size,\n",
    "                      epochs=max_epochs,\n",
    "                      validation_split=0.2,\n",
    "                      callbacks=[EarlyStopping(patience=patience_epochs)])\n",
    "  end_time = timeit.default_timer()\n",
    "\n",
    "  #EVALUATE MODEL\n",
    "  prediction=model.predict(x_test)\n",
    "  if(len(prediction.transpose())!=len(prediction)): # IF RESULT IS ONE-HOT ENCODED, CHANGE IT.\n",
    "    prediction=np.argmax(prediction,axis=1)\n",
    "\n",
    "  \n",
    "  if(metric_to_evaluate=='mae'): metric_test=mean_absolute_error(np.argmax(y_test,axis=1),prediction)\n",
    "  if(metric_to_evaluate=='accuracy'): metric_test=accuracy_score(np.argmax(y_test,axis=1),prediction)\n",
    "  if(metric_to_evaluate=='balanced_accuracy'): metric_test=balanced_accuracy_score(np.argmax(y_test,axis=1),prediction)\n",
    "  \n",
    "  #SAVE THE WEIGHTS\n",
    "  weights_name=\"{}-{}-{}-{}\".format(n_layers,input_shape,activation_function,learning_rate)\n",
    "  model.save(weights_folder+weights_name+\".h5\")\n",
    "\n",
    "  #SAVE THE HYPERPARAMS AND THE METRIC\n",
    "  with open(hp_dataset_name, mode='a+') as hp_dataset:\n",
    "      hp_dataset_writer=csv.writer(hp_dataset,delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "      hp_dataset_writer.writerow([architecture_name,\n",
    "                              problem_type,\n",
    "                              num_features,\n",
    "                              training_and_validation_samples,\n",
    "                              n_layers,\n",
    "                              input_shape,\n",
    "                              activation_function,\n",
    "                              learning_rate,\n",
    "                              batch_size,\n",
    "                              str(len(history.history['loss'])),\n",
    "                              end_time-start_time,\n",
    "                              metric_test\n",
    "                              ])\n",
    "  return metric_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def selection(evaluated_hparams,sel_prt,rand_prt,population, metric_to_evaluate,sort_order_desc):\n",
    "    \n",
    "    sorted_evaluated_params=sorted(evaluated_hparams,key=itemgetter('metric'),reverse=True)\n",
    "    if(sel_prt+rand_prt>=len(population[0])):\n",
    "      print(\"WARNING: Selections are bigger thant current population\")\n",
    "      print(\"WARNING: Random selection may not be taken\")\n",
    "\n",
    "    top_selection=[]\n",
    "    for i in range(sel_prt):\n",
    "      top_selection.insert(len(top_selection),sorted_evaluated_params[i]['hparam'])\n",
    "\n",
    "    rand_selection=[]\n",
    "    i=0\n",
    "    while(i < rand_prt):\n",
    "      if(len(rand_selection)+len(top_selection)>=len(population[0])):\n",
    "        break\n",
    "\n",
    "      rand_hparam=randrange(len(population[0]))\n",
    "      print(\"Generated random {}.\".format(rand_hparam))\n",
    "      if(rand_hparam in top_selection or rand_hparam in rand_selection):\n",
    "        continue\n",
    "\n",
    "      rand_selection.insert(0,rand_hparam)\n",
    "      i=i+1\n",
    "    return top_selection,rand_selection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CROSS-OVER OPERATION\n",
    "\n",
    "def crossover(p1,p2,population):\n",
    "    child_potential_n_layers=[]\n",
    "    child_potential_learning_rate=[]\n",
    "    child_potential_batch_size=[]\n",
    "    child_potential_activation_function=[]\n",
    "\n",
    "    #child1\n",
    "    child_potential_n_layers.insert(0,population[0][p1])\n",
    "    child_potential_learning_rate.insert(0,population[1][p2])\n",
    "    child_potential_batch_size.insert(0,population[2][p1])\n",
    "    child_potential_activation_function.insert(0,population[3][p2])\n",
    "\n",
    "    #child2\n",
    "    child_potential_n_layers.insert(0,population[0][p2])\n",
    "    child_potential_learning_rate.insert(0,population[1][p1])\n",
    "    child_potential_batch_size.insert(0,population[2][p2])\n",
    "    child_potential_activation_function.insert(0,population[3][p1])\n",
    "    \n",
    "    #child3\n",
    "    child_potential_n_layers.insert(0,population[0][p1])\n",
    "    child_potential_learning_rate.insert(0,population[1][p1])\n",
    "    child_potential_batch_size.insert(0,population[2][p2])\n",
    "    child_potential_activation_function.insert(0,population[3][p2])\n",
    "    \n",
    "    #child4\n",
    "    child_potential_n_layers.insert(0,population[0][p2])\n",
    "    child_potential_learning_rate.insert(0,population[1][p2])\n",
    "    child_potential_batch_size.insert(0,population[2][p1])\n",
    "    child_potential_activation_function.insert(0,population[3][p1])\n",
    "    \n",
    "    \n",
    "    child_hparams=[child_potential_n_layers,child_potential_learning_rate,child_potential_batch_size,child_potential_activation_function]\n",
    "    return child_hparams\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# MUTATION\n",
    "def mutation(population,selected):\n",
    "    selected_hyperparam=randrange(len(all_hyperparams))\n",
    "    selected_value=randrange(len(all_hyperparams[selected_hyperparam]))\n",
    "    population[selected_hyperparam][selected]=all_hyperparams[selected_hyperparam][selected_value]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAIN\n",
    "def genetic_algorithm_main(population_size,input_shape,hp_dataset_name,max_epochs,patience_epochs,metric_to_evaluate,sort_order_desc):\n",
    "    potential_n_layers,potential_learning_rate,potential_batch_size,potential_activation_function=initialize_population(population_size=population_size,\n",
    "                                                                                                n_layers=n_layers,\n",
    "                                                                                                learning_rate=learning_rate,\n",
    "                                                                                                batch_size=batch_size,\n",
    "                                                                                                activation_function=activation_function)\n",
    "    population=[potential_n_layers,potential_learning_rate,potential_batch_size,potential_activation_function]\n",
    "    print(\"Initial population\",population)\n",
    "    final_hyperparam=[]\n",
    "    # evaluate hyperparams\n",
    "    for generation in range(generations):\n",
    "        evaluated_hparams=[]\n",
    "        for i in range(population_size):\n",
    "            #input_shape,n_layers,activation_function,learning_rate,batch_size,hp_dataset_name,max_epochs,patience_epochs,metric_to_evaluate\n",
    "            metric=evaluate_fitness(input_shape=input_shape,\n",
    "                                    n_layers=population[0][i],\n",
    "                                    learning_rate=population[1][i],\n",
    "                                    batch_size=population[2][i],\n",
    "                                    activation_function=population[3][i],\n",
    "                                    hp_dataset_name=hp_dataset_name,\n",
    "                                    max_epochs=max_epochs,\n",
    "                                    patience_epochs=patience_epochs,\n",
    "                                    metric_to_evaluate=metric_to_evaluate)\n",
    "            evaluated_hparams.insert(0,{\"hparam\":i,\"metric\":metric})\n",
    "\n",
    "        #SELECTION\n",
    "        top_selection,rand_selection=selection(evaluated_hparams,sel_prt,rand_prt,population,metric_to_evaluate,sort_order_desc)\n",
    "\n",
    "\n",
    "        # CROSS-OVER\n",
    "        p1,p2=random.sample(range(0,len(top_selection)+len(rand_selection)),2)\n",
    "        child_hyperparams= (p1,p2,population)\n",
    "\n",
    "        #CREATE NEW POPULATION\n",
    "        #insert top selections\n",
    "        new_population=[[population[0][i] for i in top_selection],\n",
    "                          [population[1][i] for i in top_selection],\n",
    "                          [population[2][i] for i in top_selection],\n",
    "                          [population[3][i] for i in top_selection]]\n",
    "        #insert random selection and childs\n",
    "        new_population[0]=[*new_population[0],\n",
    "                                    *[population[0][i] for i in rand_selection],\n",
    "                                   *child_hyperparams[0]]\n",
    "        new_population[1]=[*new_population[1],\n",
    "                                    *[population[1][i] for i in rand_selection],\n",
    "                                   *child_hyperparams[1]]\n",
    "        new_population[2]=[*new_population[2],\n",
    "                                    *[population[2][i] for i in rand_selection],\n",
    "                                   *child_hyperparams[2]]\n",
    "        new_population[3]=[*new_population[3],\n",
    "                                    *[population[3][i] for i in rand_selection],\n",
    "                                   *child_hyperparams[3]]    \n",
    "\n",
    "        # MUTATION\n",
    "        selected_to_mutate=randrange(len(top_selection)+len(rand_selection)+len(child_hyperparams[0]))\n",
    "        mutation(new_population,selected_to_mutate)\n",
    "\n",
    "        if (generation+1)==generations:\n",
    "            for  hyperparam in  population:\n",
    "                final_hyperparam.insert(len(population),hyperparam[top_selection[0]])\n",
    "\n",
    "        population=new_population\n",
    "        population_size=len(population[0])\n",
    "\n",
    "    return evaluated_hparams,sorted(evaluated_hparams,key=itemgetter('metric'),reverse=sort_order_desc)[0]['metric'],final_hyperparam\n",
    "    \n",
    "                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n",
      "   activation_function\n",
      "0              sigmoid\n",
      "1                 relu\n",
      "2                  elu\n",
      "3              sigmoid\n",
      "4                  elu\n",
      "5                 relu\n",
      "6                  elu\n",
      "7              sigmoid\n",
      "8              sigmoid\n",
      "9                 relu\n",
      "10             sigmoid\n",
      "11                 elu\n",
      "12             sigmoid\n",
      "13                tanh\n",
      "14                relu\n",
      "15                 elu\n",
      "16             sigmoid\n",
      "17                relu\n",
      "18                 elu\n",
      "19                 elu\n",
      "20                relu\n",
      "21                 elu\n",
      "22                relu\n",
      "23                 elu\n",
      "24                relu\n",
      "25                tanh\n",
      "26             sigmoid\n",
      "27                 elu\n",
      "28                relu\n",
      "29                tanh\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Cannot interpret '<attribute 'dtype' of 'numpy.generic' objects>' as a data type",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-25496ee0e99d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0mgenerations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0mtop_lr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtop_bz\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtop_layers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtop_af\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfinish_order\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmeta_learner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtop_lr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tesis/metalearner-hptunning/Metalearner.py\u001b[0m in \u001b[0;36mmeta_learner\u001b[0;34m(n_top_hp_to_select)\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m     \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mload_meta_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_file_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdataset_column_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx_column_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mto_categorical_column_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmetric_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0merror_metric\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_metamodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m     \u001b[0mgs_population\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_hp_space\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_features\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtraining_samples\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn_layers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mactivation_function\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/tesis/metalearner-hptunning/Metalearner.py\u001b[0m in \u001b[0;36mload_meta_data\u001b[0;34m(data_file_name, dataset_column_names, x_column_names, to_categorical_column_names, metric_name, error_metric)\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx_column_names\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mto_categorical_column_names\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m     \u001b[0mdummies\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_dummies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mto_categorical_column_names\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mprefix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mprefix_sep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m     \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mto_categorical_column_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdummies\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/test/lib/python3.7/site-packages/pandas/core/reshape/reshape.py\u001b[0m in \u001b[0;36mget_dummies\u001b[0;34m(data, prefix, prefix_sep, dummy_na, columns, sparse, drop_first, dtype)\u001b[0m\n\u001b[1;32m    868\u001b[0m         \u001b[0;31m# determine columns being encoded\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    869\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcolumns\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 870\u001b[0;31m             \u001b[0mdata_to_encode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect_dtypes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minclude\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtypes_to_encode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    871\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0mdata_to_encode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/test/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mselect_dtypes\u001b[0;34m(self, include, exclude)\u001b[0m\n\u001b[1;32m   3425\u001b[0m         \u001b[0;31m# the \"union\" of the logic of case 1 and case 2:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3426\u001b[0m         \u001b[0;31m# we get the included and excluded, and return their logical and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3427\u001b[0;31m         \u001b[0minclude_these\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSeries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mnot\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minclude\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3428\u001b[0m         \u001b[0mexclude_these\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSeries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mnot\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexclude\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3429\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/test/lib/python3.7/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, index, dtype, name, copy, fastpath)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 311\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msanitize_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraise_cast_failure\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSingleBlockManager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfastpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/test/lib/python3.7/site-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36msanitize_array\u001b[0;34m(data, index, dtype, copy, raise_cast_failure)\u001b[0m\n\u001b[1;32m    710\u001b[0m                 \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmaybe_cast_to_datetime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    711\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 712\u001b[0;31m             \u001b[0msubarr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconstruct_1d_arraylike_from_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    713\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    714\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/test/lib/python3.7/site-packages/pandas/core/dtypes/cast.py\u001b[0m in \u001b[0;36mconstruct_1d_arraylike_from_scalar\u001b[0;34m(value, length, dtype)\u001b[0m\n\u001b[1;32m   1231\u001b[0m                 \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mensure_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1232\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1233\u001b[0;31m         \u001b[0msubarr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlength\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1234\u001b[0m         \u001b[0msubarr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfill\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Cannot interpret '<attribute 'dtype' of 'numpy.generic' objects>' as a data type"
     ]
    }
   ],
   "source": [
    "to_categorical_column_names=[\"activation_function\"]\n",
    "input_shape=x_train.shape[1]\n",
    "max_epochs=10\n",
    "patience_epochs=2\n",
    "metric_to_evaluate=\"balanced_accuracy\"\n",
    "sort_order_desc=True\n",
    "architecture_name=\"irnet\"\n",
    "problem_type=\"prediction\"\n",
    "#FILES NAME\n",
    "hp_dataset_name=\"test_hp_dataset.csv\"\n",
    "weights_folder=\"data/weights/\"\n",
    "# logs_file_name=\"fcunet_logs_rgs.csv\"\n",
    "# data_file_name=\"./data/1d_irnet.csv\"\n",
    "#HYPERPARAMETERS TO EVALUATE\n",
    "num_features=[29]\n",
    "training_and_validation_samples=len(x_train)\n",
    "n_layers=[1,2,3]\n",
    "learning_rate=[0.01,0.001,0.0001,0.00001]\n",
    "batch_size=[16,32,64,128]\n",
    "activation_function=['relu','elu','tanh','sigmoid']\n",
    "n_top_hp_to_select=2\n",
    "#GA configuration\n",
    "all_hyperparams=[n_layers,learning_rate,batch_size,activation_function]\n",
    "population_size=6\n",
    "sel_prt=2\n",
    "rand_prt=2\n",
    "generations=2\n",
    "\n",
    "top_lr,top_bz,top_layers,top_af,finish_order=meta_learner(2)\n",
    "print(top_lr)\n",
    "\n",
    "all_ga,top_ga, hparams_ga=genetic_algorithm_main(population_size,\n",
    "                                                input_shape,\n",
    "                                                hp_dataset_name,\n",
    "                                                max_epochs,\n",
    "                                                patience_epochs,\n",
    "                                                metric_to_evaluate,\n",
    "                                                sort_order_desc)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "NN MNIST  - GA.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.7.4 ('test': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "fb98013d4c2891267818ea7c909444c4c0da64618fb7bda0edb10d2c08cfbac9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
